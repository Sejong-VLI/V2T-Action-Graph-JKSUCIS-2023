{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "138d0222-d664-4f21-a53e-cf0bbf92d88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import pathlib\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab043ff7-cbfd-4265-8fd7-b69945bf2bbd",
   "metadata": {},
   "source": [
    "## Generate Node Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa678a2-1e85-4c52-8162-9660407e403a",
   "metadata": {},
   "source": [
    "#### Path Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02f06d43-3d1c-4ce7-bcd4-33a44d816122",
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    msvd = True # or msvd = False for MSR-VTT\n",
    "    slice_framepos = 2\n",
    "    dset ='../'\n",
    "    max_frames = 20\n",
    "    eval_frame_order = 0 \n",
    "    output_dir = 'pretrained'\n",
    "    cache_dir = ''\n",
    "    features_path = '..'\n",
    "    msrvtt_csv = 'msrvtt.csv'\n",
    "    max_words =32\n",
    "    feature_framerate = 1\n",
    "    cross_model = \"cross-base\"\n",
    "    local_rank = 0\n",
    "    pretrained_clip_name = \"ViT-B/16\" # Change to \"ViT-B/32 if you use pretrained ViT with path size 32 in CLIP4Clip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0259edd0-c50d-47a3-b445-b9725e913531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video number: 1970\n",
      "Id number: 1970\n"
     ]
    }
   ],
   "source": [
    "# MSVD\n",
    "if args.msvd:\n",
    "\n",
    "    dset_path = os.path.join(os.path.join(args.dset,'dataset'),'MSVD')\n",
    "    features_path = os.path.join(dset_path,'raw') # Raw uncompressed videos .avi    \n",
    "    name_list = glob.glob(features_path+os.sep+'*')\n",
    "    args.features_path = features_path\n",
    "\n",
    "    url2id = {}\n",
    "    data_path = os.path.join(os.path.join(dset_path,'captions','youtube_mapping.txt'))\n",
    "    args.data_path = data_path\n",
    "    for line in open(data_path,'r').readlines():\n",
    "        url2id[line.strip().split(' ')[0]] = line.strip().split(' ')[-1]\n",
    "\n",
    "    path_to_saved_models = \"extracted/msvd\"\n",
    "    pathlib.Path(path_to_saved_models).mkdir(parents=True, exist_ok=True)\n",
    "    save_file = path_to_saved_models+'/<Desired file name>.hdf5'\n",
    "    args.max_words =30\n",
    "    \n",
    "    %run ../dataloaders/dataloader_msvd_patch.py import MSVD_Loader\n",
    "    \n",
    "    videos= MSVD_Loader(\n",
    "        data_path=args.data_path,\n",
    "        features_path=args.features_path,\n",
    "        max_words=args.max_words,\n",
    "        feature_framerate=args.feature_framerate,\n",
    "        max_frames=args.max_frames,\n",
    "        frame_order=args.eval_frame_order,\n",
    "        slice_framepos=args.slice_framepos,\n",
    "        transform_type = 0,\n",
    "        patch=3,\n",
    "        overlapped=0.5\n",
    "    ) \n",
    "    \n",
    "# MSRVTT\n",
    "else:\n",
    "    dset_path = os.path.join(os.path.join(args.dset,'dataset'),'MSRVTT')\n",
    "    features_path = os.path.join(dset_path,'raw') # Raw uncompressed videos .avi\n",
    "    args.features_path = features_path\n",
    "    data_path = os.path.join(dset_path,'MSRVTT_data.json')\n",
    "    args.data_path = data_path\n",
    "    args.msrvtt_csv = os.path.join(dset_path,'msrvtt.csv')\n",
    "    name_list = glob.glob(features_path+os.sep+'*')\n",
    "\n",
    "    path_to_saved_models = \"extracted/msrvtt\"\n",
    "    pathlib.Path(path_to_saved_models).mkdir(parents=True, exist_ok=True)\n",
    "    save_file = path_to_saved_models+'/<Desired file name>.hdf5'\n",
    "    args.max_words = 73\n",
    "    \n",
    "    %run ../dataloaders/dataloader_msrvtt_patch.py import MSRVTT_RawDataLoader\n",
    "    \n",
    "    videos= MSRVTT_RawDataLoader(\n",
    "        csv_path=args.msrvtt_csv,\n",
    "        features_path=args.features_path,\n",
    "        max_words=args.max_words,\n",
    "        feature_framerate=args.feature_framerate,\n",
    "        max_frames=args.max_frames,\n",
    "        frame_order=args.eval_frame_order,\n",
    "        slice_framepos=args.slice_framepos,\n",
    "        transform_type = 0,\n",
    "        patch=3,\n",
    "        overlapped=0.5\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1784ad-c5f7-4aaf-b999-b01c46c77d83",
   "metadata": {},
   "source": [
    "#### CLIP4Clip Model Initation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "734475f2-cc2e-4c61-8ada-e1a8230ad935",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-One:True, Stage-Two:False\n",
      "\t embed_dim: 512\n",
      "\t image_resolution: 224\n",
      "\t vision_layers: 12\n",
      "\t vision_width: 768\n",
      "\t vision_patch_size: 16\n",
      "\t context_length: 77\n",
      "\t vocab_size: 49408\n",
      "\t transformer_width: 512\n",
      "\t transformer_heads: 8\n",
      "\t transformer_layers: 12\n",
      "\t cut_top_layer: 0\n"
     ]
    }
   ],
   "source": [
    "from modules.file_utils import PYTORCH_PRETRAINED_BERT_CACHE\n",
    "from modules.modeling import CLIP4Clip\n",
    "epoch = 1 # Trained CLIP4Clip best model epoch\n",
    "model_file = os.path.join(args.output_dir, \"pytorch_model.bin.{}\".format(epoch-1))\n",
    "model_state_dict = torch.load(model_file, map_location='cpu')\n",
    "cache_dir = args.cache_dir if args.cache_dir else os.path.join(str(PYTORCH_PRETRAINED_BERT_CACHE), 'distributed')\n",
    "model = CLIP4Clip.from_pretrained(args.cross_model, cache_dir=cache_dir, state_dict=model_state_dict, task_config=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab1bea7d-db21-4daf-bb88-0928e7b475e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0')\n",
    "clip = model.clip.to(device)\n",
    "clip.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9681a27c-7b11-402b-8ffc-a96b593d4500",
   "metadata": {},
   "source": [
    "#### Node Feature File Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34140290-5262-4ba5-b18a-d80556df8ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of desired pathces in the frame grid\n",
    "NUM_PATCHES = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beea967d-6c17-4045-8b9f-f49213fe7fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate node features using CLIP4Clip to extract frame representation\n",
    "with h5py.File(save_file, 'w') as f:\n",
    "    for i in tqdm(range(len(videos))):\n",
    "\n",
    "        video_id, video_patches, video_mask = videos[i]\n",
    "        length_frames = video_patches.shape[2]\n",
    "        outputs = []\n",
    "        for p in range(len(video_patches)):\n",
    "            video=video_patches[p]\n",
    "            tensor = video[0]\n",
    "            tensor = tensor[video_mask[0]==1,:]\n",
    "            tensor = torch.as_tensor(tensor).float()\n",
    "            video_frame,num,channel,h,w = tensor.shape\n",
    "            tensor = tensor.view(video_frame*num, channel, h, w)\n",
    "\n",
    "            video_frame,channel,h,w = tensor.shape\n",
    "\n",
    "            output = clip.encode_image(tensor.to(device), video_frame=video_frame).float().to(device)\n",
    "            output = output.detach().cpu().numpy()\n",
    "            outputs.append(output)\n",
    "        outputs = np.stack(outputs)\n",
    "        for o in range(len(video_mask[0])): # Iterate over frames\n",
    "            if o < outputs.shape[1]:\n",
    "                os = outputs[:, o, :]\n",
    "            else:\n",
    "                os = np.zeros((NUM_PATCHES,512)) # 512 is dimension of the extracted features\n",
    "            f.create_dataset(video_id+'-'+str(o), data = os)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
